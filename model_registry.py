"""
model_registry.py - ç®€åŒ–ç‰ˆ
ç»Ÿä¸€ API è°ƒç”¨æ¥å£ + ç¦»çº¿è¯Šæ–­æ•°æ®åº“
"""

import numpy as np
from typing import Dict, List, Optional, Any
import logging
import json
import re
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

os.environ['HTTP_PROXY'] = 'http://127.0.0.1:56054'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:56054'


# ============= ç»Ÿä¸€ API è°ƒç”¨ç±» =============

class UnifiedModelAPI:
    """
    ç»Ÿä¸€æ¨¡å‹ API è°ƒç”¨æ¥å£
    æ”¯æŒ Gemini å’Œ Baichuan ä¸¤ç§åç«¯
    """

    def __init__(self, config: Dict):
        """
        Args:
            config: {
                "gemini": {
                    "api_key": "...",
                    "model": "gemini-3-pro-vision"
                },
                "baichuan": {
                    "api_key": "...",
                    "model": "Baichuan4",
                    "api_base": "https://api.baichuan-ai.com/v1"
                }
            }
        """
        self.config = config
        self.gemini_client = None
        self.baichuan_session = None

        # æ¨¡å—åˆ°åç«¯çš„æ˜ å°„
        self.module_backend_map = {
            "locator": "gemini",      # æ„ŸçŸ¥å¯¼èˆªå™¨ -> Gemini 3 Pro
            "describer": "gemini",    # è¯­ä¹‰è§£æå‘˜ -> Gemini 3 Pro
            "reflector": "baichuan",  # å®¡æ ¸å®¡æŸ¥å‘˜ -> Baichuan
            "reporter": "baichuan"    # æŠ¥å‘Šç”Ÿæˆå™¨ -> Baichuan
        }

    def load(self):
        """åˆå§‹åŒ–æ‰€æœ‰åç«¯å®¢æˆ·ç«¯"""
        logger.info("ğŸ”§ åˆå§‹åŒ–ç»Ÿä¸€ API è°ƒç”¨æ¥å£...")

        # åˆå§‹åŒ– Gemini
        if "gemini" in self.config:
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.config["gemini"]["api_key"])
                self.gemini_client = genai.GenerativeModel(
                    self.config["gemini"].get("model", "gemini-3-pro-preview")
                )
                logger.info("  âœ… Gemini 3 Pro å·²åŠ è½½")
            except Exception as e:
                logger.error(f"  âŒ Gemini åŠ è½½å¤±è´¥: {e}")

        # åˆå§‹åŒ– Baichuan
        if "baichuan" in self.config:
            try:
                # TODO: å–æ¶ˆæ³¨é‡Šä»¥ä½¿ç”¨çœŸå® Baichuan
                import requests
                import json
                self.baichuan_session = requests.Session()
                self.baichuan_session.headers.update({
                    "Authorization": f"Bearer {self.config['baichuan']['api_key']}",
                    "Content-Type": "application/json"
                })
                logger.info("  âœ… Baichuan 3 å·²åŠ è½½")
            except Exception as e:
                logger.error(f"  âŒ Baichuan åŠ è½½å¤±è´¥: {e}")

        logger.info("âœ… ç»Ÿä¸€ API æ¥å£åˆå§‹åŒ–å®Œæˆ\n")

    def call(self,
             module: str,
             prompt: str,
             image: Optional[np.ndarray] = None,
             **kwargs) -> str:
        """
        ç»Ÿä¸€è°ƒç”¨æ¥å£

        Args:
            module: æ¨¡å—åç§° ("locator" | "describer" | "reflector" | "reporter")
            prompt: æ–‡æœ¬æç¤º
            image: å›¾åƒæ•°æ®ï¼ˆå¯é€‰ï¼Œä»… Vision æ¨¡å‹éœ€è¦ï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚ temperature, max_tokensï¼‰

        Returns:
            str: æ¨¡å‹å“åº”æ–‡æœ¬
        """
        backend = self.module_backend_map.get(module)

        if backend == "gemini":
            return self._call_gemini(prompt, image, **kwargs)
        elif backend == "baichuan":
            return self._call_baichuan(prompt, **kwargs)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å—: {module}")

    def _call_gemini(self,
                     prompt: str,
                     image: Optional[np.ndarray] = None,
                     **kwargs) -> str:
        """è°ƒç”¨ Gemini API"""

        # TODO: å®é™… API è°ƒç”¨
        from PIL import Image

        if image is not None:
            img = Image.fromarray(image)
            response = self.gemini_client.generate_content([prompt, img])
        else:
            response = self.gemini_client.generate_content(prompt)

        return response.text

        # Mock å“åº”
        # logger.info("  ğŸ¤– [Gemini] æ¨¡æ‹Ÿè°ƒç”¨...")
        # if "ROI" in prompt or "æ£€æµ‹" in prompt:
        #     return json.dumps({
        #         "rois": [
        #             {"center_x": 5000, "center_y": 8000, "bbox": [4800, 7800, 5200, 8200],
        #              "confidence": 0.92, "class": "tumor_region"},
        #             {"center_x": 12000, "center_y": 6000, "bbox": [11800, 5800, 12200, 6200],
        #              "confidence": 0.87, "class": "dysplastic_area"}
        #         ]
        #     })
        # elif "å½¢æ€å­¦" in prompt or "æè¿°" in prompt:
        #     return json.dumps({
        #         "ç»†èƒç‰¹å¾": "æ ¸æµ†æ¯”å¢é«˜(>1:2)ï¼Œæ ¸åˆ†è£‚è±¡ 3-5/HPFï¼Œæ ¸ä»æ˜æ˜¾",
        #         "ç»„ç»‡ç»“æ„": "è…ºä½“èåˆæ’åˆ—ï¼ŒèƒŒé èƒŒæ¨¡å¼ï¼Œå±€éƒ¨åæ­»",
        #         "é—´è´¨æ”¹å˜": "é—´è´¨çº¤ç»´åŒ–ä¼´æ·‹å·´ç»†èƒæµ¸æ¶¦",
        #         "åŸºåº•è†œ": "åŸºåº•è†œå±€éƒ¨ä¸­æ–­ï¼Œä¾µçŠ¯ç²˜è†œä¸‹å±‚",
        #         "completeness_score": 0.95
        #     })
        # else:
        #     return "Gemini mock response"

    def _call_baichuan(self, prompt: str, **kwargs) -> str:
        """è°ƒç”¨ Baichuan API"""

        # TODO: å®é™… API è°ƒç”¨
        response = self.baichuan_session.post(
            f"https://api.baichuan-ai.com/v1/chat/completions",
            json={
                "model": self.config['baichuan'].get('model', 'Baichuan-M3'),
                "messages": [{"role": "user", "content": prompt}],
                "temperature": kwargs.get("temperature", 0.7),
                "max_tokens": kwargs.get("max_tokens", 32000)
            }
        )
        result = response.json()
        return result["choices"][0]["message"]["content"]

        # Mock å“åº”
        # logger.info("  ğŸ¤– [Baichuan] æ¨¡æ‹Ÿè°ƒç”¨...")
        # if "å®¡æŸ¥" in prompt or "è´¨é‡" in prompt:
        #     return json.dumps({
        #         "quality_score": 0.95,
        #         "missing_fields": [],
        #         "action": "PROCEED",
        #         "suggestions": "æè¿°å®Œæ•´ï¼Œå¯è¿›å…¥è¯Šæ–­é˜¶æ®µ"
        #     })
        # elif "æŠ¥å‘Š" in prompt:
        #     return """
        #         === ç—…ç†è¯Šæ–­æŠ¥å‘Š ===
        #
        #         ã€æ ‡æœ¬ä¿¡æ¯ã€‘
        #         æ¥æºç»„ç»‡ï¼šèƒƒçª¦ç²˜è†œæ´»æ£€
        #         æŸ“è‰²æ–¹æ³•ï¼šHE æŸ“è‰²
        #
        #         ã€é•œä¸‹æ‰€è§ã€‘
        #         è…ºä½“èåˆæ’åˆ—ï¼ŒèƒŒé èƒŒæ¨¡å¼ï¼Œå±€éƒ¨åæ­»
        #
        #         ã€è¯Šæ–­æ„è§ã€‘
        #         è‚¿ç˜¤åˆ†å‹ï¼šä¸­åˆ†åŒ–è…ºç™Œ
        #         æµ¸æ¶¦æ·±åº¦ï¼šè‚Œå±‚ (2.3 mm)
        #
        #         ã€ç—…ç†åˆ†æœŸå»ºè®®ã€‘
        #         T2 (ä¾µçŠ¯è‚Œå±‚)
        #         """
        # else:
        #     return "Baichuan mock response"

    @staticmethod
    def parse_json_response(text: str) -> Dict:
        """ä»å“åº”æ–‡æœ¬ä¸­æå– JSON"""
        # å°è¯•æå– JSON ä»£ç å—
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            text = json_match.group(1)

        # å°è¯•è§£æ JSON
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆ JSONï¼Œè¿”å›åŸæ–‡æœ¬
            return {"raw_text": text}


# ============= Prompt æ¨¡æ¿åº“ =============

class PromptTemplates:
    """é¢„å®šä¹‰çš„ Prompt æ¨¡æ¿"""

    LOCATOR_PROMPT = """
        ä½ æ˜¯ç—…ç†å­¦ä¸“å®¶ã€‚è¯·åˆ†æè¿™å¼ ä½å€ç‡ç—…ç†åˆ‡ç‰‡ç¼©ç•¥å›¾ï¼Œè¯†åˆ«æ‰€æœ‰å¯ç–‘çš„è‚¿ç˜¤åŒºåŸŸï¼ˆROIï¼‰ã€‚
        
        å¯¹æ¯ä¸ª ROIï¼Œè¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
        ```json
        {
          "rois": [
            {
              "center_x": åƒç´ åæ ‡,
              "center_y": åƒç´ åæ ‡,
              "bbox": [x1, y1, x2, y2],
              "confidence": 0-1ä¹‹é—´çš„ç½®ä¿¡åº¦,
              "class": "tumor_region" | "dysplastic_area" | "inflammatory_area"
            }
          ]
        }
        ```
        
        é‡ç‚¹å…³æ³¨ï¼šç»†èƒå¯†é›†åŒºã€ç»“æ„ç´Šä¹±åŒºã€å¼‚å‹ç»†èƒèšé›†åŒºã€‚
        åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
        """

    DESCRIBER_PROMPT = """
        ä½ æ˜¯èµ„æ·±ç—…ç†ä¸“ç§‘åŒ»ç”Ÿã€‚è¯·åˆ†æè¯¥ç—…ç†åˆ‡ç‰‡çš„é«˜å€ç‡å›¾åƒï¼Œå¿…é¡»æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼š
        
        ã€ç»†èƒç‰¹å¾ã€‘ï¼šæ ¸æµ†æ¯”ã€æ ¸åˆ†è£‚è±¡ã€æ ¸ä»çŠ¶æ€
        ã€ç»„ç»‡ç»“æ„ã€‘ï¼šè…ºä½“æ’åˆ—ã€åæ­»æƒ…å†µã€åˆ†åŒ–ç¨‹åº¦
        ã€é—´è´¨æ”¹å˜ã€‘ï¼šçº¤ç»´åŒ–ã€ç‚æ€§æµ¸æ¶¦ã€è¡€ç®¡çŠ¶æ€
        ã€åŸºåº•è†œã€‘ï¼šè¿ç»­æ€§ã€ç ´åç¨‹åº¦ã€ä¾µçŠ¯æ·±åº¦
        
        è¦æ±‚ï¼š
        - æ¯é¡¹è‡³å°‘ 15 å­—è¯¦ç»†æè¿°
        - ä½¿ç”¨æ ‡å‡†ç—…ç†æœ¯è¯­
        - å®¢è§‚æè¿°ï¼Œé¿å…è¯Šæ–­æ€§ç»“è®º
        
        è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
        ```json
        {
          "ç»†èƒç‰¹å¾": "...",
          "ç»„ç»‡ç»“æ„": "...",
          "é—´è´¨æ”¹å˜": "...",
          "åŸºåº•è†œ": "...",
          "completeness_score": 0.0-1.0
        }
        ```
        
        åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
        """

    @staticmethod
    def reflector_prompt(description: Dict, diagnostic_goal: str = "subtype+invasion") -> str:
        return f"""
            ä½ æ˜¯ç—…ç†è´¨æ§ä¸“å®¶ã€‚è¯·å®¡æŸ¥ä»¥ä¸‹å½¢æ€å­¦æè¿°çš„è´¨é‡ã€‚
            
            è¯Šæ–­ç›®æ ‡ï¼š{diagnostic_goal}
            
            æè¿°å†…å®¹ï¼š
            {json.dumps(description, ensure_ascii=False, indent=2)}
            
            è¯·è¯„ä¼°ï¼š
            1. æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼ˆç»†èƒç‰¹å¾ã€ç»„ç»‡ç»“æ„ã€é—´è´¨æ”¹å˜ã€åŸºåº•è†œï¼‰
            2. æ¯ä¸ªå­—æ®µçš„æè¿°æ˜¯å¦è¶³å¤Ÿè¯¦ç»†ï¼ˆè‡³å°‘15å­—ï¼‰
            3. æ˜¯å¦ä½¿ç”¨äº†æ ‡å‡†ç—…ç†æœ¯è¯­
            
            ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
            ```json
            {{
              "quality_score": 0-1ä¹‹é—´çš„åˆ†æ•°,
              "missing_fields": ["ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„å­—æ®µ"],
              "action": "RE-SCAN" æˆ– "PROCEED",
              "suggestions": "å…·ä½“æ”¹è¿›å»ºè®®"
            }}
            ```
            
            åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
        """

    @staticmethod
    def reporter_prompt(observations: List[Dict], diagnostics: Dict, slide_id: str) -> str:
        return f"""
            ä½ æ˜¯ç—…ç†æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆç¬¦åˆä¸´åºŠè§„èŒƒçš„ç—…ç†è¯Šæ–­æŠ¥å‘Šã€‚
            
            åˆ‡ç‰‡ç¼–å·ï¼š{slide_id}
            
            é•œä¸‹è§‚å¯Ÿç»“æœï¼š
            {json.dumps(observations, ensure_ascii=False, indent=2)}
            
            è¯Šæ–­ç»“è®ºï¼š
            {json.dumps(diagnostics, ensure_ascii=False, indent=2)}
            
            è¦æ±‚ï¼š
            1. ä½¿ç”¨è§„èŒƒçš„ç—…ç†æŠ¥å‘Šæ ¼å¼
            2. åŒ…å«ï¼šæ ‡æœ¬ä¿¡æ¯ã€é•œä¸‹æ‰€è§ã€è¯Šæ–­æ„è§ã€åˆ†æœŸå»ºè®®
            3. è¯­è¨€ä¸“ä¸šã€å‡†ç¡®ã€ç®€æ´
            4. é¿å…è¿‡åº¦è§£è¯»ï¼Œå®¢è§‚æè¿°äº‹å®
            
            è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„ç—…ç†æŠ¥å‘Šï¼Œä¸è¦ JSON æ ¼å¼ã€‚
            """


# ============= ç¦»çº¿è¯Šæ–­æ•°æ®åº“ (Mock) =============

class DiagnosisDatabase:
    """ç¦»çº¿è¯Šæ–­æ•°æ®åº“æ¥å£ï¼ˆMock å®ç°ï¼‰"""

    def __init__(self, db_config: Optional[Dict] = None):
        self.db_config = db_config or {"type": "mock"}
        self.mock_data = {}  # æ¨¡æ‹Ÿæ•°æ®å­˜å‚¨

    def load(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ”§ è¿æ¥è¯Šæ–­æ•°æ®åº“...")

        # é¢„å¡«å……ä¸€äº› Mock æ•°æ®
        self.mock_data = {
            "SLIDE-001_(5000, 8000)": {
                "subtype": "moderately_differentiated_adenocarcinoma",
                "subtype_confidence": 0.89,
                "invasion_layer": "muscularis_propria",
                "depth_mm": 2.3,
                "invasion_confidence": 0.91,
                "model_version": "virchow2_atlas2_v2.1"
            },
            "SLIDE-001_(12000, 6000)": {
                "subtype": "well_differentiated_adenocarcinoma",
                "subtype_confidence": 0.92,
                "invasion_layer": "submucosa",
                "depth_mm": 1.2,
                "invasion_confidence": 0.88,
                "model_version": "virchow2_atlas2_v2.1"
            }
        }

        logger.info(f"  âœ… æ•°æ®åº“å·²è¿æ¥ (ç±»å‹: Mock, ç¼“å­˜æ•°: {len(self.mock_data)})\n")

    def query(self, slide_id: str, roi_coord: tuple) -> Optional[Dict]:
        """
        æŸ¥è¯¢è¯Šæ–­ç»“æœ

        Args:
            slide_id: åˆ‡ç‰‡ç¼–å·
            roi_coord: ROI åæ ‡ (x, y)

        Returns:
            Dict: è¯Šæ–­ç»“æœæˆ– None
        """
        key = f"{slide_id}_{roi_coord}"
        result = self.mock_data.get(key)

        if result:
            logger.info(f"  âœ… [æ•°æ®åº“] å‘½ä¸­ç¼“å­˜: {roi_coord} -> {result['subtype']}")
        else:
            logger.warning(f"  âš ï¸ [æ•°æ®åº“] æœªå‘½ä¸­ç¼“å­˜: {key}")
            # è¿”å›é»˜è®¤ç»“æœ
            result = {
                "subtype": "undetermined",
                "subtype_confidence": 0.0,
                "invasion_layer": "unknown",
                "depth_mm": 0.0,
                "invasion_confidence": 0.0,
                "model_version": "mock_fallback"
            }

        return result

    def batch_query(self, slide_id: str, roi_list: List[tuple]) -> List[Dict]:
        """æ‰¹é‡æŸ¥è¯¢"""
        return [self.query(slide_id, coord) for coord in roi_list]


# ============= æ¨¡å‹æ³¨å†Œä¸­å¿ƒï¼ˆç®€åŒ–ç‰ˆï¼‰ =============

class ModelRegistry:
    """
    ç®€åŒ–ç‰ˆæ¨¡å‹æ³¨å†Œä¸­å¿ƒ
    ç»Ÿä¸€ç®¡ç† API è°ƒç”¨å’Œæ•°æ®åº“æŸ¥è¯¢
    """

    def __init__(self, config: Dict):
        """
        Args:
            config: {
                "api": {
                    "gemini": {"api_key": "...", "model": "gemini-3-pro-vision"},
                    "baichuan": {"api_key": "...", "model": "Baichuan4"}
                },
                "database": {"type": "mock"}
            }
        """
        self.config = config

        # åˆå§‹åŒ–ç»„ä»¶
        self.api = UnifiedModelAPI(config.get("api", {}))
        self.database = DiagnosisDatabase(config.get("database", {}))
        self.prompts = PromptTemplates()

    def load_all(self):
        """åŠ è½½æ‰€æœ‰ç»„ä»¶"""
        logger.info("="*70)
        logger.info("ğŸš€ åˆå§‹åŒ–ç—…ç†è¯Šæ–­ç³»ç»Ÿ")
        logger.info("="*70 + "\n")

        self.api.load()
        self.database.load()

        logger.info("="*70)
        logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info("="*70 + "\n")

    # ----------- ä¾¿æ·è°ƒç”¨æ–¹æ³• -----------

    def detect_rois(self, thumbnail: np.ndarray) -> List[Dict]:
        """æ„ŸçŸ¥å¯¼èˆªå™¨ï¼šæ£€æµ‹ ROI"""
        logger.info("ğŸ” [Locator] æ£€æµ‹ ROI...")
        response = self.api.call("locator", self.prompts.LOCATOR_PROMPT, image=thumbnail)
        result = self.api.parse_json_response(response)
        return result.get("rois", [])

    def describe_patch(self, patch: np.ndarray) -> Dict:
        """è¯­ä¹‰è§£æå‘˜ï¼šç”Ÿæˆå½¢æ€å­¦æè¿°"""
        logger.info("ğŸ”¬ [Describer] ç”Ÿæˆå½¢æ€å­¦æè¿°...")
        response = self.api.call("describer", self.prompts.DESCRIBER_PROMPT, image=patch)
        return self.api.parse_json_response(response)

    def reflect_quality(self, description: Dict, goal: str = "subtype+invasion") -> Dict:
        """å®¡æ ¸å®¡æŸ¥å‘˜ï¼šè´¨é‡åæ€"""
        logger.info("ğŸ¤” [Reflector] å®¡æŸ¥æè¿°è´¨é‡...")
        prompt = self.prompts.reflector_prompt(description, goal)
        response = self.api.call("reflector", prompt)
        return self.api.parse_json_response(response)

    def generate_report(self, observations: List[Dict], diagnostics: Dict, slide_id: str) -> str:
        """æŠ¥å‘Šç”Ÿæˆå™¨ï¼šç”Ÿæˆç—…ç†æŠ¥å‘Š"""
        logger.info("ğŸ“„ [Reporter] ç”Ÿæˆç—…ç†æŠ¥å‘Š...")
        prompt = self.prompts.reporter_prompt(observations, diagnostics, slide_id)
        return self.api.call("reporter", prompt)

    def query_diagnosis(self, slide_id: str, roi_coord: tuple) -> Dict:
        """æŸ¥è¯¢ç¦»çº¿è¯Šæ–­ç»“æœ"""
        logger.info(f"ğŸ—„ï¸ [Database] æŸ¥è¯¢è¯Šæ–­ç»“æœ: {roi_coord}")
        return self.database.query(slide_id, roi_coord)


# ============= ä½¿ç”¨ç¤ºä¾‹ =============

if __name__ == "__main__":
    # é…ç½®
    config = {
        "api": {
            "gemini": {
                "api_key": "your-gemini-api-key",
                "model": "gemini-3-pro-vision"
            },
            "baichuan": {
                "api_key": "your-baichuan-api-key",
                "model": "Baichuan4",
                "api_base": "https://api.baichuan-ai.com/v1"
            }
        },
        "database": {
            "type": "mock"
        }
    }

    # åˆå§‹åŒ–
    registry = ModelRegistry(config)
    registry.load_all()

    # æµ‹è¯•æ•°æ®
    dummy_thumbnail = np.random.randint(0, 255, (2048, 2048, 3), dtype=np.uint8)
    dummy_patch = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)

    print("\n" + "="*70)
    print("æµ‹è¯•å„ä¸ªæ¨¡å—")
    print("="*70 + "\n")

    # 1. ROI æ£€æµ‹
    rois = registry.detect_rois(dummy_thumbnail)
    print(f"âœ… æ£€æµ‹åˆ° {len(rois)} ä¸ª ROI\n")

    # 2. å½¢æ€å­¦æè¿°
    description = registry.describe_patch(dummy_patch)
    print(f"âœ… æè¿°å®Œæ•´åº¦: {description.get('completeness_score', 0):.2f}\n")

    # 3. è´¨é‡åæ€
    reflection = registry.reflect_quality(description)
    print(f"âœ… è´¨é‡è¯„åˆ†: {reflection.get('quality_score', 0):.2f}\n")

    # 4. æŸ¥è¯¢è¯Šæ–­
    diagnosis = registry.query_diagnosis("SLIDE-001", (5000, 8000))
    print(f"âœ… è¯Šæ–­: {diagnosis['subtype']}\n")

    # 5. ç”ŸæˆæŠ¥å‘Š
    observations = [{"description": description, "roi_type": "tumor_region"}]
    report = registry.generate_report(observations, diagnosis, "SLIDE-001")
    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ\n")
    print(report)